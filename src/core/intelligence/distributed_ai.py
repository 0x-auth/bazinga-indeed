#!/usr/bin/env python3
"""
distributed_ai.py - Distributed BAZINGA AI

This is the distributed version of BAZINGA that:
1. Uses FREE LLM APIs (Groq, Together.ai, HuggingFace)
2. Can run on GitHub Actions / any cloud
3. Shares knowledge via IPFS (future)
4. No central control - intelligence emerges from the network

Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              DISTRIBUTED BAZINGA                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Local Node          â”‚    Remote APIs               â”‚
    â”‚  â”œâ”€ ChromaDB         â”‚    â”œâ”€ Groq (free, fast)     â”‚
    â”‚  â”œâ”€ Embeddings       â”‚    â”œâ”€ Together.ai           â”‚
    â”‚  â””â”€ Ï†-Coherence      â”‚    â”œâ”€ HuggingFace           â”‚
    â”‚                      â”‚    â””â”€ OpenRouter            â”‚
    â”‚         â†“            â”‚           â†“                  â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚    â”‚   Unified Response with Ï†-filter    â”‚         â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

"Intelligence distributed, not controlled"

Author: Built for Space (Abhishek/Abhilasia)
Date: 2025-02-09
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib

# Add parent paths
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

# BAZINGA imports
from src.core.lambda_g import LambdaGOperator, PHI

# Constants
ALPHA = 137
PROGRESSION = '01âˆâˆ«âˆ‚âˆ‡Ï€Ï†Î£Î”Î©Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰'


@dataclass
class LLMProvider:
    """Configuration for an LLM provider."""
    name: str
    base_url: str
    api_key_env: str  # Environment variable name for API key
    model: str
    is_free: bool
    rate_limit: int  # Requests per minute
    priority: int  # Lower = try first


# Free LLM Providers (no API key needed for some, or free tier)
FREE_PROVIDERS = [
    LLMProvider(
        name="Groq",
        base_url="https://api.groq.com/openai/v1",
        api_key_env="GROQ_API_KEY",
        model="llama-3.1-8b-instant",
        is_free=True,
        rate_limit=30,
        priority=1
    ),
    LLMProvider(
        name="Together",
        base_url="https://api.together.xyz/v1",
        api_key_env="TOGETHER_API_KEY",
        model="meta-llama/Llama-3.2-3B-Instruct-Turbo",
        is_free=True,
        rate_limit=60,
        priority=2
    ),
    LLMProvider(
        name="OpenRouter",
        base_url="https://openrouter.ai/api/v1",
        api_key_env="OPENROUTER_API_KEY",
        model="meta-llama/llama-3.1-8b-instruct:free",
        is_free=True,
        rate_limit=20,
        priority=3
    ),
    LLMProvider(
        name="HuggingFace",
        base_url="https://api-inference.huggingface.co/models",
        api_key_env="HF_TOKEN",
        model="meta-llama/Llama-3.2-3B-Instruct",
        is_free=True,
        rate_limit=10,
        priority=4
    ),
]


class DistributedLLM:
    """
    Distributed LLM client that tries multiple free providers.

    Falls back through providers if one fails or is rate-limited.
    Uses Ï†-coherence to select best response when multiple succeed.
    """

    def __init__(self):
        self.lambda_g = LambdaGOperator()
        self.providers = self._get_available_providers()
        self.request_counts: Dict[str, int] = {}
        self.last_reset = datetime.now()

        print(f"\nğŸŒ Distributed LLM initialized with {len(self.providers)} providers:")
        for p in self.providers:
            has_key = bool(os.environ.get(p.api_key_env))
            status = "âœ“ configured" if has_key else "â—‹ needs API key"
            print(f"   {p.name}: {status}")
        print()

    def _get_available_providers(self) -> List[LLMProvider]:
        """Get providers sorted by priority."""
        # Return all providers, sorted by priority
        # We'll check for API keys when making requests
        return sorted(FREE_PROVIDERS, key=lambda p: p.priority)

    def _get_api_key(self, provider: LLMProvider) -> Optional[str]:
        """Get API key for a provider."""
        return os.environ.get(provider.api_key_env)

    async def _call_provider(
        self,
        provider: LLMProvider,
        prompt: str,
        system_prompt: str,
        timeout: float = 30.0
    ) -> Optional[str]:
        """Call a single LLM provider."""
        api_key = self._get_api_key(provider)
        if not api_key:
            return None

        if not HTTPX_AVAILABLE:
            return None

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        # OpenRouter needs extra headers
        if provider.name == "OpenRouter":
            headers["HTTP-Referer"] = "https://github.com/bazinga-ai"
            headers["X-Title"] = "BAZINGA Distributed AI"

        # Different payload format for HuggingFace
        if provider.name == "HuggingFace":
            payload = {
                "inputs": f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:",
                "parameters": {
                    "max_new_tokens": 500,
                    "temperature": 0.7,
                }
            }
            url = f"{provider.base_url}/{provider.model}"
        else:
            payload = {
                "model": provider.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 500,
            }
            url = f"{provider.base_url}/chat/completions"

        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.post(url, headers=headers, json=payload)

                if response.status_code == 200:
                    data = response.json()

                    # Parse response based on provider
                    if provider.name == "HuggingFace":
                        if isinstance(data, list) and len(data) > 0:
                            return data[0].get("generated_text", "")
                    else:
                        choices = data.get("choices", [])
                        if choices:
                            return choices[0].get("message", {}).get("content", "")

                elif response.status_code == 429:
                    print(f"   âš ï¸  {provider.name}: Rate limited")
                else:
                    print(f"   âš ï¸  {provider.name}: Error {response.status_code}")

        except Exception as e:
            print(f"   âš ï¸  {provider.name}: {type(e).__name__}")

        return None

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        try_all: bool = False
    ) -> Tuple[str, str]:
        """
        Generate response using distributed LLM providers.

        Args:
            prompt: The user prompt
            system_prompt: Optional system prompt
            try_all: If True, try all providers and pick best by coherence

        Returns:
            Tuple of (response, provider_name)
        """
        if not system_prompt:
            system_prompt = """You are BAZINGA, a distributed AI assistant.
You provide helpful, accurate, and concise responses.
Your intelligence emerges from a network of knowledge, not a single source."""

        responses: List[Tuple[str, str, float]] = []  # (response, provider, coherence)

        for provider in self.providers:
            if not self._get_api_key(provider):
                continue

            print(f"   Trying {provider.name}...")
            response = await self._call_provider(provider, prompt, system_prompt)

            if response:
                # Calculate coherence
                coherence = self.lambda_g.calculate_coherence(response[:500])
                responses.append((response, provider.name, coherence.total_coherence))

                if not try_all:
                    # Return first successful response
                    return response, provider.name

        if responses:
            # Pick response with highest coherence
            responses.sort(key=lambda x: x[2], reverse=True)
            best = responses[0]
            return best[0], best[1]

        return "No LLM providers available. Please set API keys.", "none"


class DistributedAI:
    """
    BAZINGA Distributed AI - Intelligence that runs anywhere.

    This version:
    1. Works without local Ollama
    2. Uses free cloud LLM APIs
    3. Can be deployed to GitHub Actions, Vercel, etc.
    4. Maintains Ï†-coherence filtering
    5. Ready for P2P extension (Phase 2)
    """

    VERSION = "1.0.0-distributed"

    def __init__(self, use_local_kb: bool = True):
        """
        Initialize Distributed BAZINGA.

        Args:
            use_local_kb: If True, also use local ChromaDB knowledge base
        """
        self.lambda_g = LambdaGOperator()
        self.llm = DistributedLLM()
        self.local_ai = None

        # Optionally load local knowledge base
        if use_local_kb:
            try:
                from src.core.intelligence.real_ai import RealAI
                self.local_ai = RealAI()
                print("ğŸ“š Local knowledge base loaded")
            except Exception as e:
                print(f"âš ï¸  Local KB not available: {e}")

        self._print_banner()

    def _print_banner(self):
        """Print banner."""
        print()
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘                                                                  â•‘")
        print("â•‘   âŸ¨Ïˆ|Î›|Î©âŸ©  BAZINGA DISTRIBUTED - DECENTRALIZED AI  âŸ¨Ïˆ|Î›|Î©âŸ©      â•‘")
        print("â•‘                                                                  â•‘")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        print("â•‘                                                                  â•‘")
        print("â•‘   'Intelligence distributed, not controlled'                     â•‘")
        print("â•‘                                                                  â•‘")
        print("â•‘   Features:                                                      â•‘")
        print("â•‘     â€¢ Multiple FREE LLM providers (Groq, Together, etc.)         â•‘")
        print("â•‘     â€¢ Ï†-coherence filtering on all responses                     â•‘")
        print("â•‘     â€¢ Local + Cloud knowledge fusion                             â•‘")
        print("â•‘     â€¢ Ready for P2P / IPFS (Phase 2)                             â•‘")
        print("â•‘                                                                  â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print()

    async def ask(
        self,
        question: str,
        use_local_context: bool = True,
        verbose: bool = True
    ) -> str:
        """
        Ask a question to distributed BAZINGA.

        Combines:
        1. Local knowledge base context (if available)
        2. Distributed LLM generation
        3. Ï†-coherence filtering

        Args:
            question: Your question
            use_local_context: Include local KB context
            verbose: Print debug info
        """
        if verbose:
            print(f"\nğŸ” Question: {question}")

        # Build context from local KB
        context = ""
        sources = []

        if use_local_context and self.local_ai:
            results = self.local_ai.search(question, limit=5)
            if results:
                context_parts = []
                for r in results[:3]:
                    context_parts.append(r.chunk.content[:500])
                    sources.append(Path(r.chunk.source_file).name)
                context = "\n\n".join(context_parts)
                if verbose:
                    print(f"   Found {len(results)} local context chunks")

        # Build prompt
        if context:
            prompt = f"""Based on this context from the knowledge base:

{context}

Answer this question: {question}

Be concise and helpful. Cite sources when relevant."""
        else:
            prompt = question

        # Generate response
        if verbose:
            print("   Generating response...")

        response, provider = await self.llm.generate(prompt)

        if verbose:
            print(f"   âœ“ Response from: {provider}")

        # Calculate coherence
        coherence = self.lambda_g.calculate_coherence(response[:500])

        if verbose:
            print(f"   Ï†-coherence: {coherence.total_coherence:.3f}")

        # Format final response
        if sources:
            source_list = ", ".join(set(sources[:3]))
            response = f"{response}\n\nğŸ“š Sources: {source_list}"

        if coherence.is_vac:
            response = f"{response}\n\nâœ¨ V.A.C. achieved in response!"

        return response

    async def interactive(self):
        """Run interactive mode."""
        print("\nâ—Š DISTRIBUTED BAZINGA - INTERACTIVE MODE â—Š")
        print("-" * 50)
        print("Commands:")
        print("  /providers  - Show available LLM providers")
        print("  /coherence  - Toggle coherence display")
        print("  /quit       - Exit")
        print("-" * 50)
        print()

        show_coherence = True

        while True:
            try:
                query = input("You: ").strip()

                if not query:
                    continue

                if query.lower() in ['/quit', '/exit', '/q']:
                    print("\nâœ¨ BAZINGA distributed network signing off.")
                    break

                if query == '/providers':
                    print("\nAvailable providers:")
                    for p in self.llm.providers:
                        has_key = bool(os.environ.get(p.api_key_env))
                        status = "âœ“" if has_key else "âœ—"
                        print(f"  {status} {p.name} ({p.model})")
                    print()
                    continue

                if query == '/coherence':
                    show_coherence = not show_coherence
                    print(f"Coherence display: {'on' if show_coherence else 'off'}")
                    continue

                response = await self.ask(query, verbose=show_coherence)
                print(f"\nBAZINGA: {response}\n")

            except KeyboardInterrupt:
                print("\n\nâœ¨ BAZINGA signing off.")
                break
            except EOFError:
                break


def get_setup_instructions() -> str:
    """Get instructions for setting up API keys."""
    return """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 SETUP FREE LLM API KEYS                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  1. GROQ (Recommended - Fast & Free):                            â•‘
â•‘     â†’ Go to: https://console.groq.com/keys                       â•‘
â•‘     â†’ Create free account, get API key                           â•‘
â•‘     â†’ Run: export GROQ_API_KEY="your-key"                        â•‘
â•‘                                                                  â•‘
â•‘  2. Together.ai (Good free tier):                                â•‘
â•‘     â†’ Go to: https://api.together.xyz/settings/api-keys          â•‘
â•‘     â†’ Create account, get API key                                â•‘
â•‘     â†’ Run: export TOGETHER_API_KEY="your-key"                    â•‘
â•‘                                                                  â•‘
â•‘  3. OpenRouter (Many free models):                               â•‘
â•‘     â†’ Go to: https://openrouter.ai/keys                          â•‘
â•‘     â†’ Create account, get API key                                â•‘
â•‘     â†’ Run: export OPENROUTER_API_KEY="your-key"                  â•‘
â•‘                                                                  â•‘
â•‘  Add to ~/.bashrc or ~/.bashrc for persistence.                   â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


async def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(
        description="BAZINGA Distributed AI - Decentralized Intelligence"
    )
    parser.add_argument('--ask', type=str, help='Ask a question')
    parser.add_argument('--setup', action='store_true', help='Show setup instructions')
    parser.add_argument('--no-local', action='store_true', help='Skip local KB')

    args = parser.parse_args()

    if args.setup:
        print(get_setup_instructions())
        return

    # Check if any API keys are set
    has_any_key = any(
        os.environ.get(p.api_key_env)
        for p in FREE_PROVIDERS
    )

    if not has_any_key:
        print(get_setup_instructions())
        print("\nâš ï¸  No API keys found. Set at least one to use distributed mode.")
        print("   Or use ./run_bazinga.sh for local-only mode.\n")
        return

    # Initialize
    ai = DistributedAI(use_local_kb=not args.no_local)

    if args.ask:
        response = await ai.ask(args.ask)
        print(f"\n{response}\n")
    else:
        await ai.interactive()


if __name__ == "__main__":
    asyncio.run(main())
