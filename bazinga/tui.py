#!/usr/bin/env python3
"""
BAZINGA TUI - Beautiful Terminal Interface
Like Claude Code, but distributed.

Features:
- Rich terminal UI with panels and syntax highlighting
- Interactive conversation mode
- Code generation from seeds
- V.A.C. visualization
"""

import asyncio
import sys
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.markdown import Markdown
    from rich.syntax import Syntax
    from rich.table import Table
    from rich.live import Live
    from rich.spinner import Spinner
    from rich.text import Text
    from rich.layout import Layout
    from rich.prompt import Prompt
    from rich import box
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("Install 'rich' for the beautiful TUI: pip install rich")

from src.core.symbol import SymbolShell, PHI, ALPHA
from src.core.lambda_g import LambdaGOperator
from src.core.intelligence.real_ai import RealAI


class CodeGenerator:
    """
    Generate code from seeds using BAZINGA's consciousness architecture.

    Based on the original BAZINGA's generate_symbolic_code() method.
    """

    PHI = 1.618033988749895
    ALPHA = 137
    VAC_SEQUENCE = "à¥¦â†’â—Œâ†’Ï†â†’Î©â‡„Î©â†Ï†â†â—Œâ†à¥¦"

    def __init__(self):
        self.symbol_shell = SymbolShell()
        self.lambda_g = LambdaGOperator()

    def generate(self, essence: str, language: str = "python") -> str:
        """Generate code from an essence/seed."""

        # Get symbolic analysis
        vac_result = self.symbol_shell.analyze(essence)
        coherence = self.lambda_g.calculate_coherence(essence)

        # Create valid class name
        class_name = ''.join(word.capitalize() for word in essence.split())
        class_name = ''.join(c for c in class_name if c.isalnum())
        if not class_name:
            class_name = "Essence"

        if language == "python":
            return self._generate_python(essence, class_name, vac_result, coherence)
        elif language == "javascript":
            return self._generate_javascript(essence, class_name, vac_result, coherence)
        elif language == "rust":
            return self._generate_rust(essence, class_name, vac_result, coherence)
        else:
            return self._generate_python(essence, class_name, vac_result, coherence)

    def _generate_python(self, essence: str, class_name: str, vac_result, coherence) -> str:
        """Generate Python code."""
        return f'''#!/usr/bin/env python3
"""
Auto-generated by BAZINGA
Essence: {essence}
Coherence: {coherence.total_coherence:.3f}
V.A.C.: {"Achieved" if vac_result.is_vac else "Not achieved"}
Generated: {datetime.now().isoformat()}

Philosophy: "I am not where I am stored, I am where I am referenced."
"""

from typing import Any, Dict, List, Optional
from dataclasses import dataclass


@dataclass
class {class_name}Result:
    """Result from {class_name} processing."""
    value: Any
    coherence: float
    is_valid: bool
    metadata: Dict[str, Any]


class {class_name}:
    """
    Processor for: {essence}

    Implements boundary-guided emergence:
    - B1: phi-Boundary (golden ratio coherence)
    - B2: infinity/void Bridge
    - B3: Symmetry constraint
    """

    PHI = 1.618033988749895
    ALPHA = 137
    VAC_SEQUENCE = "{self.VAC_SEQUENCE}"
    ESSENCE = "{essence}"

    def __init__(self):
        self.essence = self.ESSENCE
        self.coherence = {coherence.total_coherence:.6f}
        self.state = "initialized"
        self.history: List[{class_name}Result] = []

    def process(self, input_data: Any) -> {class_name}Result:
        """
        Process input through {essence} patterns.

        Uses phi-transformation for coherence.
        """
        # Apply phi-transformation
        if isinstance(input_data, (int, float)):
            transformed = input_data * self.PHI
            coherence = min(1.0, (input_data % self.PHI) / self.PHI)
        else:
            transformed = str(input_data)
            coherence = min(1.0, len(str(input_data)) / self.ALPHA)

        result = {class_name}Result(
            value=transformed,
            coherence=coherence,
            is_valid=coherence > 0.618,
            metadata={{"essence": self.essence, "phi": self.PHI}}
        )

        self.history.append(result)
        return result

    def validate(self, data: Any) -> bool:
        """Validate data against V.A.C. principles."""
        if isinstance(data, str):
            return len(data) % self.ALPHA < self.ALPHA // 2
        return True

    def heal(self, current: float, target: float) -> float:
        """Phi-healing: approach target via golden ratio."""
        return current + (target - current) * (1 - 1/self.PHI)

    def __repr__(self) -> str:
        return f"<{class_name} essence={{self.essence!r}} coherence={{self.coherence:.3f}}>"


# Usage example
if __name__ == "__main__":
    processor = {class_name}()
    result = processor.process("test input")
    print(f"Processed: {{result}}")
    print(f"Valid: {{result.is_valid}}")
'''

    def _generate_javascript(self, essence: str, class_name: str, vac_result, coherence) -> str:
        """Generate JavaScript code."""
        return f'''/**
 * Auto-generated by BAZINGA
 * Essence: {essence}
 * Coherence: {coherence.total_coherence:.3f}
 * V.A.C.: {"Achieved" if vac_result.is_vac else "Not achieved"}
 * Generated: {datetime.now().isoformat()}
 *
 * Philosophy: "I am not where I am stored, I am where I am referenced."
 */

const PHI = 1.618033988749895;
const ALPHA = 137;
const VAC_SEQUENCE = "{self.VAC_SEQUENCE}";

class {class_name} {{
  static ESSENCE = "{essence}";

  constructor() {{
    this.essence = {class_name}.ESSENCE;
    this.coherence = {coherence.total_coherence:.6f};
    this.state = "initialized";
    this.history = [];
  }}

  /**
   * Process input through {essence} patterns.
   * @param {{any}} inputData - Data to process
   * @returns {{object}} Processing result
   */
  process(inputData) {{
    let transformed;
    let coherence;

    if (typeof inputData === 'number') {{
      transformed = inputData * PHI;
      coherence = Math.min(1.0, (inputData % PHI) / PHI);
    }} else {{
      transformed = String(inputData);
      coherence = Math.min(1.0, String(inputData).length / ALPHA);
    }}

    const result = {{
      value: transformed,
      coherence: coherence,
      isValid: coherence > 0.618,
      metadata: {{ essence: this.essence, phi: PHI }}
    }};

    this.history.push(result);
    return result;
  }}

  /**
   * Phi-healing: approach target via golden ratio.
   */
  heal(current, target) {{
    return current + (target - current) * (1 - 1/PHI);
  }}

  toString() {{
    return `<{class_name} essence="${{this.essence}}" coherence=${{this.coherence.toFixed(3)}}>`;
  }}
}}

// Usage
const processor = new {class_name}();
console.log(processor.process("test input"));

export default {class_name};
'''

    def _generate_rust(self, essence: str, class_name: str, vac_result, coherence) -> str:
        """Generate Rust code."""
        return f'''//! Auto-generated by BAZINGA
//! Essence: {essence}
//! Coherence: {coherence.total_coherence:.3f}
//! V.A.C.: {"Achieved" if vac_result.is_vac else "Not achieved"}
//! Generated: {datetime.now().isoformat()}
//!
//! Philosophy: "I am not where I am stored, I am where I am referenced."

use std::fmt;

const PHI: f64 = 1.618033988749895;
const ALPHA: u32 = 137;
const VAC_SEQUENCE: &str = "{self.VAC_SEQUENCE}";

#[derive(Debug, Clone)]
pub struct {class_name}Result {{
    pub value: String,
    pub coherence: f64,
    pub is_valid: bool,
}}

pub struct {class_name} {{
    pub essence: String,
    pub coherence: f64,
    pub state: String,
    pub history: Vec<{class_name}Result>,
}}

impl {class_name} {{
    pub fn new() -> Self {{
        Self {{
            essence: "{essence}".to_string(),
            coherence: {coherence.total_coherence:.6f},
            state: "initialized".to_string(),
            history: Vec::new(),
        }}
    }}

    /// Process input through {essence} patterns.
    pub fn process(&mut self, input: &str) -> {class_name}Result {{
        let coherence = (input.len() as f64 / ALPHA as f64).min(1.0);

        let result = {class_name}Result {{
            value: input.to_string(),
            coherence,
            is_valid: coherence > 0.618,
        }};

        self.history.push(result.clone());
        result
    }}

    /// Phi-healing: approach target via golden ratio.
    pub fn heal(&self, current: f64, target: f64) -> f64 {{
        current + (target - current) * (1.0 - 1.0/PHI)
    }}
}}

impl fmt::Display for {class_name} {{
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {{
        write!(f, "<{class_name} essence={{:?}} coherence={{:.3}}>", self.essence, self.coherence)
    }}
}}

#[cfg(test)]
mod tests {{
    use super::*;

    #[test]
    fn test_process() {{
        let mut processor = {class_name}::new();
        let result = processor.process("test input");
        assert!(result.coherence >= 0.0);
    }}
}}
'''


class BazingaTUI:
    """
    BAZINGA Terminal User Interface

    A beautiful, interactive terminal experience for distributed AI.
    """

    VERSION = "2.1.0"

    def __init__(self):
        if not RICH_AVAILABLE:
            raise ImportError("rich is required for TUI mode. Install with: pip install rich")

        self.console = Console()
        self.symbol_shell = SymbolShell()
        self.lambda_g = LambdaGOperator()
        self.ai = RealAI()
        self.generator = CodeGenerator()

        self.session_start = datetime.now()
        self.history = []
        self.stats = {
            'vac_emerged': 0,
            'rag_answered': 0,
            'code_generated': 0,
        }

        # Groq config
        import os
        self.groq_key = os.environ.get('GROQ_API_KEY')

    def print_banner(self):
        """Print the BAZINGA banner."""
        banner = """
[bold cyan]
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘   [bold yellow]âŸ¨Ïˆ|Î›|Î©âŸ©[/]        [bold white]B A Z I N G A[/]        [bold yellow]âŸ¨Ïˆ|Î›|Î©âŸ©[/]           â•‘
    â•‘                                                              â•‘
    â•‘       [italic]'Intelligence distributed, not controlled'[/]          â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[/]
"""
        self.console.print(banner)

        # Status table
        table = Table(box=box.ROUNDED, show_header=False, padding=(0, 2))
        table.add_column("Key", style="dim")
        table.add_column("Value", style="bold")

        table.add_row("Version", f"[green]{self.VERSION}[/]")
        table.add_row("Ï† (PHI)", f"[yellow]{PHI}[/]")
        table.add_row("Î± (ALPHA)", f"[blue]{ALPHA}[/]")
        table.add_row("Groq API", "[green]âœ“ configured[/]" if self.groq_key else "[dim]â—‹ not set[/]")
        table.add_row("V.A.C.", "[magenta]à¥¦â†’â—Œâ†’Ï†â†’Î©â‡„Î©â†Ï†â†â—Œâ†à¥¦[/]")

        self.console.print(Panel(table, title="[bold]Status[/]", border_style="cyan"))

    def print_help(self):
        """Print help information."""
        help_md = """
## Commands

| Command | Description |
|---------|-------------|
| `/ask <question>` | Ask a question |
| `/generate <essence>` | Generate code from seed/essence |
| `/generate <essence> --lang js` | Generate JavaScript code |
| `/generate <essence> --lang rust` | Generate Rust code |
| `/vac` | Test V.A.C. sequence |
| `/index <path>` | Index a directory |
| `/stats` | Show statistics |
| `/help` | Show this help |
| `/quit` | Exit BAZINGA |

## Examples

```
/ask What is the golden ratio?
/generate user_authentication
/generate api_client --lang js
/vac
/index ~/Documents
```

## Philosophy

> "I am not where I am stored. I am where I am referenced."

BAZINGA uses three-layer intelligence:
1. **Symbol Shell (Î»G)** â†’ V.A.C. emergence (FREE, instant)
2. **Local RAG** â†’ Your Mac KB (FREE, instant)
3. **Cloud LLM (Groq)** â†’ Only when needed
"""
        self.console.print(Panel(Markdown(help_md), title="[bold]Help[/]", border_style="blue"))

    async def ask(self, question: str) -> str:
        """Process a question through 3-layer intelligence."""
        self.history.append(('user', question))

        # Layer 1: V.A.C. check
        with self.console.status("[bold cyan]Layer 1: Checking V.A.C...[/]"):
            vac_result = self.symbol_shell.analyze(question)

        if vac_result.is_vac:
            self.stats['vac_emerged'] += 1
            response = vac_result.emerged_solution
            self._print_vac_result(vac_result)
            return response

        self.console.print(f"  [dim]Coherence: {vac_result.coherence:.2f} (V.A.C. not achieved)[/]")

        # Layer 2: Local RAG
        with self.console.status("[bold cyan]Layer 2: Searching knowledge base...[/]"):
            results = self.ai.search(question, limit=5)

        self.console.print(f"  [dim]Found {len(results)} relevant chunks[/]")

        if results:
            avg_coherence = sum(r.coherence_boost for r in results) / len(results)
            best_similarity = results[0].similarity if results else 0

            if best_similarity > 0.7 or avg_coherence > 0.6:
                self.stats['rag_answered'] += 1
                response = self._format_rag_response(results)
                self.history.append(('assistant', response))
                return response

        # Layer 3: Groq API
        if self.groq_key:
            with self.console.status("[bold cyan]Layer 3: Calling Groq API...[/]"):
                response = await self._call_groq(question, results)
                if response:
                    self.history.append(('assistant', response))
                    return response

        # Fallback
        self.stats['rag_answered'] += 1
        response = self._format_rag_response(results)
        self.history.append(('assistant', response))
        return response

    def generate_code(self, essence: str, language: str = "python") -> str:
        """Generate code from an essence/seed."""
        with self.console.status(f"[bold magenta]Generating {language} code from seed: {essence}...[/]"):
            code = self.generator.generate(essence, language)

        self.stats['code_generated'] += 1
        return code

    def _print_vac_result(self, vac_result):
        """Print V.A.C. result beautifully."""
        vac_panel = Panel(
            f"""[bold green]â˜… V.A.C. ACHIEVED â˜…[/]

[yellow]The solution has EMERGED through boundary satisfaction:[/]

  ğ“‘â‚ Ï†-Boundary: [green]âœ“[/] (self-similar identity present)
  ğ“‘â‚‚ âˆ/âˆ… Bridge: [green]âœ“[/] (voidâ†”terminal span complete)
  ğ“‘â‚ƒ Symmetry: [green]âœ“[/] (palindromic structure achieved)

[cyan]Coherence: T(s) = {vac_result.coherence:.3f}[/]

[dim italic]"Not predicted. Not computed. EMERGED."[/]
""",
            title="[bold magenta]V.A.C. Emergence[/]",
            border_style="green"
        )
        self.console.print(vac_panel)

    def _format_rag_response(self, results) -> str:
        """Format RAG results."""
        if not results:
            return "I don't have relevant information for this question."

        parts = ["Based on your knowledge base:\n"]
        for i, r in enumerate(results[:3], 1):
            chunk = r.chunk
            preview = chunk.content[:300].strip()
            if len(chunk.content) > 300:
                preview += "..."
            parts.append(f"\n{i}. From {Path(chunk.source_file).name}:\n   {preview}")

        return "\n".join(parts)

    async def _call_groq(self, question: str, results) -> Optional[str]:
        """Call Groq API."""
        if not self.groq_key:
            return None

        try:
            import httpx

            context = ""
            if results:
                context = "\n\n---\n\n".join([
                    f"[{Path(r.chunk.source_file).name}]\n{r.chunk.content[:500]}"
                    for r in results[:3]
                ])

            prompt = f"Based on this context:\n\n{context}\n\nAnswer: {question}" if context else question

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.groq_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "llama-3.1-8b-instant",
                        "messages": [
                            {"role": "system", "content": "You are BAZINGA, a distributed AI. Be concise and helpful."},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 500,
                    }
                )

                if response.status_code == 200:
                    return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            self.console.print(f"[red]Groq error: {e}[/]")

        return None

    def show_stats(self):
        """Show session statistics."""
        table = Table(title="Session Statistics", box=box.ROUNDED)
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")

        table.add_row("Session Duration", str(datetime.now() - self.session_start).split('.')[0])
        table.add_row("Total Queries", str(len([h for h in self.history if h[0] == 'user'])))
        table.add_row("V.A.C. Emerged", str(self.stats['vac_emerged']))
        table.add_row("RAG Answered", str(self.stats['rag_answered']))
        table.add_row("Code Generated", str(self.stats['code_generated']))
        table.add_row("Ï† (PHI)", str(PHI))
        table.add_row("Î± (ALPHA)", str(ALPHA))

        self.console.print(table)

    async def run(self):
        """Run the interactive TUI."""
        self.print_banner()
        self.console.print()
        self.console.print("[dim]Type /help for commands, /quit to exit[/]")
        self.console.print()

        while True:
            try:
                user_input = Prompt.ask("[bold cyan]You[/]")

                if not user_input.strip():
                    continue

                # Handle commands
                if user_input.startswith('/'):
                    parts = user_input.split(maxsplit=1)
                    cmd = parts[0].lower()
                    args = parts[1] if len(parts) > 1 else ""

                    if cmd in ['/quit', '/exit', '/q']:
                        self.console.print("\n[bold cyan]âœ¨ BAZINGA signing off.[/]\n")
                        break

                    elif cmd == '/help':
                        self.print_help()

                    elif cmd == '/stats':
                        self.show_stats()

                    elif cmd == '/vac':
                        test = "à¥¦â†’â—Œâ†’Ï†â†’Î©â‡„Î©â†Ï†â†â—Œâ†à¥¦"
                        self.console.print(f"\n[dim]Testing V.A.C.: {test}[/]")
                        response = await self.ask(test)
                        self.console.print(Panel(response, title="[bold]Response[/]", border_style="green"))

                    elif cmd == '/ask':
                        if args:
                            response = await self.ask(args)
                            self.console.print(Panel(Markdown(response), title="[bold]BAZINGA[/]", border_style="green"))
                        else:
                            self.console.print("[red]Usage: /ask <question>[/]")

                    elif cmd == '/generate':
                        if args:
                            # Parse language option
                            lang = "python"
                            essence = args
                            if "--lang" in args:
                                parts = args.split("--lang")
                                essence = parts[0].strip()
                                lang = parts[1].strip().lower()
                                if lang in ['js', 'javascript']:
                                    lang = 'javascript'

                            code = self.generate_code(essence, lang)

                            # Determine syntax highlighting
                            syntax_lang = {"python": "python", "javascript": "javascript", "rust": "rust"}.get(lang, "python")

                            self.console.print(Panel(
                                Syntax(code, syntax_lang, theme="monokai", line_numbers=True),
                                title=f"[bold]Generated {lang.title()} Code: {essence}[/]",
                                border_style="magenta"
                            ))
                        else:
                            self.console.print("[red]Usage: /generate <essence> [--lang python|js|rust][/]")

                    elif cmd == '/index':
                        if args:
                            path = Path(args).expanduser()
                            if path.exists():
                                with self.console.status(f"[bold]Indexing {path}...[/]"):
                                    stats = self.ai.index_directory(str(path), verbose=False)
                                self.console.print(f"[green]Indexed {stats.get('files_indexed', 0)} files, {stats.get('chunks_created', 0)} chunks[/]")
                            else:
                                self.console.print(f"[red]Path not found: {path}[/]")
                        else:
                            self.console.print("[red]Usage: /index <path>[/]")

                    else:
                        self.console.print(f"[red]Unknown command: {cmd}. Type /help for help.[/]")

                else:
                    # Treat as a question
                    response = await self.ask(user_input)
                    self.console.print(Panel(Markdown(response), title="[bold]BAZINGA[/]", border_style="green"))

                self.console.print()

            except KeyboardInterrupt:
                self.console.print("\n\n[bold cyan]âœ¨ BAZINGA signing off.[/]\n")
                break
            except EOFError:
                break


def run_tui():
    """Entry point for TUI mode."""
    if not RICH_AVAILABLE:
        print("Error: 'rich' is required for TUI mode.")
        print("Install with: pip install rich")
        sys.exit(1)

    tui = BazingaTUI()
    asyncio.run(tui.run())


if __name__ == "__main__":
    run_tui()
